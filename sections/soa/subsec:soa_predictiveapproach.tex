Several researches focused on predictive approaches such as, Association rules mining, ANN based algorithm, Simple Logistic, 
 Random Forest, Logistic regression analysis, ICRM2.\cite{mduma_survey_2019}. 
In this paper, we are going to analyse the following models \cite{mduma_survey_2019,quinlan_induction_1986,yadav_mining_2012,heredia_student_2015,ramirez_prediction_2018,cox_regression_1958,perez_modelo_2018,pandey_data_2011,cover_nearest_1967,mardolkar_forecasting_2020,zhang_neural_2000,rudin_stop_2019,siri_predicting_2015,m_alban_she_is_with_the_faculty_of_engineering_and_applied_sciences_of_the_technical_university_cotopaxi_neural_2019,boser_training_1992,lee_machine_2019,behr_early_2020,friedman_stochastic_2002,eckert_analysis_2015,tenpipat_student_2020,liang_machine_2016,liang_big_2016,fischer_angulo_modelo_2012,miranda_analysis_2017,viloria_integration_2019,kemper_predicting_2020,agrusti_university_2019}:
\begin{enumerate}
\item Decision Trees
\item Logistic Regression
\item K-Nearest Neighbors (KNN)
\item Neural Networks
%\item Support Vector Machine
\item Random Forest
\end{enumerate}
From this review paper \cite{agrusti_university_2019}, we can extract this following table of these different techniques and how often they were used in other research paper.
\begin{table}[H]
    \centering
    \caption{CLASSIFICATION TECHNIQUES frequencies\cite{agrusti_university_2019}}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Techniques} & \textbf{Frequency}\\
        \hline
        Decision Tree & 49\\
        \hline
        Neural Networks & 29\\
        \hline
        Logistic regression & 25\\
        \hline
        KNN & 9\\
        \hline
    \end{tabular}
    \label{tab:class_tech_freq_agrusti}
\end{table}
These models have been chosen as they have been the most recurring ones within the literature. We will analyse each models to list the pros and cons of each one and to determine which one(s) we should use to build our predictive system.


\vspace{8pt}
\paragraph{Decision Tree}
The Decision Tree model, in ML, stand out as a fundamental and versatile algorithm. It is widely recognized for their simplicity and efficacy for classification and regression tasks alike.
Their tree-like structure gave them their name to the algorithm, comprised of nodes and branches (symbolizing decisions and their possible consequences). This helpful visualization and interpretation makes complex decision-making processes easier and intuitive for the user. This visualization is really powerful and is an invaluable tool in various applications, ranging from data mining to advanced research in AI.

Decision trees have been used in a wide range of fields such the medical, game-theory weather prediction and much more.\cite{quinlan_induction_1986}.
From the literature, decision tree seems to have a precision of around 83\% based on multiple papers (73\% \cite{viloria_integration_2019}, 87.27\% \cite{ramirez_prediction_2018}, 83\%\cite{kemper_predicting_2020} and around 90\% \cite{tenpipat_student_2020}). This research paper has evaluated multiple algorithm within decision tree and graphed out a ROC Curve for each of their algorithm. In a similar domain as this study, which is students dropout in South Korea schools and how to predict them. Their ROC curves showed that for a low rate of false positive, we get an outstanding true positive rate. Approximately attaining their limits around 0.2 false positive rate\cite{lee_machine_2019}. This study shows the predictive efficiency of the decision tree model. 
One downside of each study is the granularity of student prediction. They have shown which variable can be used to determine which kind of students may be at risk of dropout, but they lack a micro vision to alert staff about a specific student at risk.

\vspace{8pt}
\paragraph{Logistic Regression}
Logistic regression, different from linear regression by its ability to handle binary outcomes (scenarios where the outcome is dichotomous), is an important part of the statistical method in the field of ML. It is particularly interesting for its role in classification tasks. This method works by modeling the probability of a binary response, based on one or more predicator variables (independent variables). 
Our scenario of student dropout is not dichotomous by nature. However, this method can be a first step for data cleansing and classification of at risk student and out of risk students.

An interesting approach by this study \cite{lan_sparse_2014} - which change the granularity of the prediction to a student based one - is to create a “grade book” (a source of information commonly used in the context of classical test theory \cite{novick_axioms_1966}), filed with ones if the student answer correctly to question \textit{i} or 0 if not. Then, weights are added to each question depending on their difficulty to define if a student is at risk of failing and dropout. However, this approach does include some sort of e-learning method, and does solve the problem from a per-student level. Yet, it does not take into account other factors cited above to paint a bigger picture of a life of a student, and thus, defining if this student is at risk or not. It could simply be failing this course, and even if a pattern arises for a specific student (of failing), can we conclude he is at risk of dropping out?

\vspace{8pt}
\paragraph{K-Nearest Neighbors (KNN)}
K-Nearest Neighbors (KNN) is fundamental in the field of ML. As a non-parametric, instance-based learning algorithm, it is particularly renewed for its simplicity and effectiveness for tasks such as classification and regression.
The core principle of KNN is to predict labels of new data points by looking at the closest labelled data points 'K' and - for classification, take a majority vote - or an average in the case of regression. This algorithm basically learns by itself with the more data it is fed.
It can be really powerful in our case, as it excels in scenarios where the decision boundary is irregular and blurry.

The following study \cite{shiful_machine_2021}, compared to other methods, was the second most accurate models from all other models (apart from the Random Forest algorithm).
\begin{table}[H]
    \centering
    \caption{Training and testing Accuracy\cite{shiful_machine_2021}}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Model name} & \textbf{Training accuracy}  & \textbf{Testing accuracy}\\
        \hline
        Decision Tree & 80\% & 80\% \\
        \hline
        KNN & 83\% & 84\% \\
        \hline
        Random forest & 94\% & 86\% \\
        \hline
    \end{tabular}
    \label{tab:training_testing_acc_shiful}
\end{table}

This review shows us that, even though it  might not be the most accurate model (at least in this study \cite{shiful_machine_2021}), it is an interesting model to chose for comparing students between them, and potentially discover groupings of students which may be at risk of dropout.

From this same study, the searchers have compared each classifier for each model. For KNN, they found these results :
\begin{table}[H]
    \centering
    \caption{Comparison of all classifier\cite{shiful_machine_2021}}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Classifier} & & \textbf{Precision} & \textbf{recall}\\
        \hline
        KNN & Not Dropout & 86\% & 84\% \\
        \cline{2-4} 
        & Dropout & 69\% & 26\% \\
        \hline
    \end{tabular}
    \label{tab:compar_classifier_shiful}
\end{table}

This result implies that the model is pretty accurate, with an estimated 85\% of accuracy with these new results.

\vspace{8pt}
\paragraph{Neural Networks}
Neural network envision the AI and ML landscape by taking inspiration from the human brain's structure and function. A network is composed of layers of interconnected neurons, each capable of performing simple probability computations. Thanks to these layers of computations, a Neural Network can learn complex patterns and relationships in data from the input. This makes the incredibly powerful for a wide range of tasks. 
This technique turned into a field of its own, which now includes different structured Neural Network such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), each useful for specific types of dataset and tasks. Their learning capacity and ability make them an incredible powerful tool in the field of AI and ML. 

Neural Network is a vast field in itself. This first study used the Perceptron Multilayer algorithm with an admissible error of 0.0001 and using the hyperbolic tangent function for activating input and output layers\cite{viloria_integration_2019}. They have estimated that the classification was 83\% accurate according to the ROC curve of the neural network classifiers.
\begin{table}[H]
    \centering
    \caption{Comparison of the classifier evaluation parameters\cite{viloria_integration_2019}}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Model name} & \textbf{Precision}  & \textbf{Recall}\\
        \hline
        Decision Tree & 72\% & 64\% \\
        \hline
        Neural network & 73\% & 65\% \\
        \hline
    \end{tabular}
    \label{tab:comparaison_classifier_eval_param_viloria}
\end{table}

This new table of comparison of classifier per model shows us one more time that we cannot establish if one is better than the other or not. Once more, each model has is particularities, advantages, and disadvantages. 

The perceptron Multilayer algorithm is the most commonly used algorithm within the field of ML and AI.\cite{siri_predicting_2015} For this research, they have study :
\begin{quote}
    "This study asked two research questions: 
    \begin{enumerate}
        \item How accurately do pre-entry students’ characteristics predict the risk of dropout?
        \item Which characteristics weigh most in predicting the risk of dropout?"
    \end{enumerate} 
\end{quote}\cite{siri_predicting_2015}
For both question, they have concluded that the model is a pilot instrument and may be enhanced. They advised that :
\begin{quote}
    "Future research should consider the extension of the study to other groups of students not only enrolled in the healthcare area because it could reveal the importance of including additional variables not considered in this research."
\end{quote}\cite{siri_predicting_2015}

In this other study \cite{siri_predicting_2015}, their neural network model had a prediction accuracy of about 84\% for their first study group, 81\% for group 2 and 76\% for the dropout group. 

We will see in the conclusion of this literature review, which model seems to be the more fitted for our problem, taking into account the complexity and cost of each model as a system.
\vspace{8pt}
%\subsubsection{Support Vector Machine (SVM)}
%Support Vector Machines (SVM) are a powerful and versatile method for classification and regression tasks. 

%Support Vector Machines (SVM) are a powerful and versatile class of supervised learning algorithms, widely recognized for their robustness and efficacy in classification and regression tasks. Central to the SVM methodology is the concept of finding the optimal hyperplane that best separates different classes in the feature space. This is achieved by maximizing the margin between the data points of different classes, which are represented as vectors in this space. SVMs are particularly adept at handling high-dimensional data and are known for their ability to manage overfitting, even in complex datasets with a large number of features. The flexibility of SVMs is further exemplified through the use of kernel functions, which allow them to operate in a transformed feature space, enabling the handling of non-linear relationships. This makes SVMs highly effective for a wide range of applications, from text classification to bioinformatics. As we explore the current landscape of machine learning, understanding the nuances of SVMs, including their theoretical foundations, practical implementations, and the scenarios where they excel, is essential for appreciating their significant role in advancing the field of predictive modeling and data analysis.

%\textit{\textbf{W.I.P}}
%\vspace{8pt}$
\paragraph{Random Forest}
Another good method for classification and regression tasks, Random Forest, is a robust and versatile learning method. It operated by creating a multitude of decision trees during training, then outputting the class - for classification, the mode of the classes - for regression, the mean prediction (for each individual tree).
This technique enhance the predictive accuracy and overcomes the overfitting problem associated with single decision tree.
Its strength lies in its ability to handle large datasets without extensive data preprocessing requirements. 

One student focused on different Random Forest algorithm, including RF using the synthetic minority oversampling techniques (SMOTE) or Boosted RF. It showed interesting premises with from a predictive standpoint. \cite{lee_machine_2019}
\begin{table}[H]
    \centering
    \caption{AUC results of the classifiers used in this study\cite{lee_machine_2019}}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Model} & \textbf{AUC}\\
        \hline
        Random Forest & 0.986 \\
        \hline
        RF + SMOTE & 0.986\\
        \hline
        Boosted RF & 0.988 \\
        \hline
    \end{tabular}
    \label{tab:auc_values_lee}
\end{table}

Boosted RF is an advanced version of RF using boosting. Boosting is a method of sequentially improving a model's accuracy by focusing on previous models' misclassification. The algorithm starts with a base RF and iteratively adds more trees. Each new tree trained on the residuals of the previous ensemble of trees. This process continues until a specified number of trees are added, or no significant improvement is observed.

SMOTE is a powerful approach used in ML. Primarily used when dealing with imbalanced datasets (datasets where one class, usually the class of interest, is underrepresented compared to other classes). These datasets lead to poorly performing and biased models.
However, it's important to apply SMOTE carefully, as it can introduce noise into the dataset. This approach is best used with a large dataset and when the minority class is underrepresented in order to create meaningful synthetic samples.